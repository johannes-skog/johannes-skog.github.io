<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Johannes Skog</title>
    <link>https://johannes-skog.github.io/</link>
    <description>Recent content on Johannes Skog</description>
    <image>
      <url>https://johannes-skog.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://johannes-skog.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 18 Nov 2022 20:52:02 +0100</lastBuildDate><atom:link href="https://johannes-skog.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>General</title>
      <link>https://johannes-skog.github.io/post/deeplearning/</link>
      <pubDate>Fri, 18 Nov 2022 20:52:02 +0100</pubDate>
      
      <guid>https://johannes-skog.github.io/post/deeplearning/</guid>
      <description>Glossary &amp;amp; Concepts Inductive Bias Inductive reasoning from philosophy, from samples observatations generalize beyond.
Exploting known symertries in order to mitigate &amp;ldquo;curse of dimentionallity&amp;rdquo;.
DL model very data hungry - we are using very few prior, dobule edge swords (learn very complex patterns outside of our knowlege base from where we can draw priors from.)
We are using some prior knowledge about the problem domain in order to construct a model that follows the rules/limits of the problem domain.</description>
    </item>
    
    <item>
      <title>Diffusion model</title>
      <link>https://johannes-skog.github.io/post/diffusion/</link>
      <pubDate>Tue, 15 Nov 2022 20:52:02 +0100</pubDate>
      
      <guid>https://johannes-skog.github.io/post/diffusion/</guid>
      <description>In this post I will write about diffusion models, diffusion model has gotten an upswing with recent successes e.g. (DALL-E). Diffusion models falls in the class of generative models, compared to GANs they are much easier to train and do not require the adversarial setup that can be tricky to get right. There are however some drawbacks, while GANs can produce a one-time inference to get the final results, the diffusion model must traverse a chain of multiple (in some cases $&amp;gt; 4000$) inference steps in order to end up with the final results.</description>
    </item>
    
    <item>
      <title>Autoencoders &amp; Variational Autoencoder</title>
      <link>https://johannes-skog.github.io/post/autoencoders/</link>
      <pubDate>Sun, 06 Nov 2022 20:52:02 +0100</pubDate>
      
      <guid>https://johannes-skog.github.io/post/autoencoders/</guid>
      <description>In this post I will write about Autoencoders and Variational Autoencoders, where the former is used to compress some data into a dense representation that can be used in various applications, the latter extends Autoencodes with additional properties which makes it possible to generate data that looks to follow the original data distribution. Both of these fall into the class of Latent Variable Models.
Latent Variable Models Latent Variable Models is a class of models that maps an observable variable $X$ to a latent (hidden/unknown) variable $Z$.</description>
    </item>
    
    <item>
      <title>About me</title>
      <link>https://johannes-skog.github.io/page/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://johannes-skog.github.io/page/about/</guid>
      <description>Contact me on LinkedIn or via johannes.skog.unsec@gmail.com (not checked frequently)</description>
    </item>
    
  </channel>
</rss>
