<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Training Large (+7b) Language Models on Chat Data: Using DeepSpeed and LORA for Efficient Training | Johannes Skog</title>
<meta name=keywords content>
<meta name=description content="In this post, I will go through the process of training a large language model on chat data, specifically using the LLaMA-7b model. The fine-tuned model has been shown to perform on par or better than most Hugging Face variants when trained on cleaned alpaca data. I will go into the benefits of using DeepSpeed for training and how LORA (Low-Rank Adaptation) can be used in combination with DeepSpeed to be very efficient during supervised fine-tuning on large models.">
<meta name=author content="Johannes Skog">
<link rel=canonical href=https://johannes-skog.github.io/post/llm-chat/>
<meta name=google-site-verification content="XYZabc">
<meta name=yandex-verification content="XYZabc">
<meta name=msvalidate.01 content="XYZabc">
<link crossorigin=anonymous href=/assets/css/stylesheet.2b2689f619f5ca0e2da45710b8e56846446fb899d5e011fa5ccb6a8131af1349.css integrity="sha256-KyaJ9hn1yg4tpFcQuOVoRkRvuJnV4BH6XMtqgTGvE0k=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://johannes-skog.github.io/%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=16x16 href=https://johannes-skog.github.io/%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=32x32 href=https://johannes-skog.github.io/%3Clink%20/%20abs%20url%3E>
<link rel=apple-touch-icon href=https://johannes-skog.github.io/%3Clink%20/%20abs%20url%3E>
<link rel=mask-icon href=https://johannes-skog.github.io/%3Clink%20/%20abs%20url%3E>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script type=text/javascript>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-123-45','auto'),ga('send','pageview'))</script><meta property="og:title" content="Training Large (+7b) Language Models on Chat Data: Using DeepSpeed and LORA for Efficient Training">
<meta property="og:description" content="In this post, I will go through the process of training a large language model on chat data, specifically using the LLaMA-7b model. The fine-tuned model has been shown to perform on par or better than most Hugging Face variants when trained on cleaned alpaca data. I will go into the benefits of using DeepSpeed for training and how LORA (Low-Rank Adaptation) can be used in combination with DeepSpeed to be very efficient during supervised fine-tuning on large models.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://johannes-skog.github.io/post/llm-chat/"><meta property="og:image" content="https://johannes-skog.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="post">
<meta property="article:published_time" content="2023-04-25T00:00:00+00:00">
<meta property="article:modified_time" content="2023-04-25T00:00:00+00:00"><meta property="og:site_name" content="Johannes Skog">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://johannes-skog.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E">
<meta name=twitter:title content="Training Large (+7b) Language Models on Chat Data: Using DeepSpeed and LORA for Efficient Training">
<meta name=twitter:description content="In this post, I will go through the process of training a large language model on chat data, specifically using the LLaMA-7b model. The fine-tuned model has been shown to perform on par or better than most Hugging Face variants when trained on cleaned alpaca data. I will go into the benefits of using DeepSpeed for training and how LORA (Low-Rank Adaptation) can be used in combination with DeepSpeed to be very efficient during supervised fine-tuning on large models.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://johannes-skog.github.io/post/"},{"@type":"ListItem","position":2,"name":"Training Large (+7b) Language Models on Chat Data: Using DeepSpeed and LORA for Efficient Training","item":"https://johannes-skog.github.io/post/llm-chat/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Training Large (+7b) Language Models on Chat Data: Using DeepSpeed and LORA for Efficient Training","name":"Training Large (\u002b7b) Language Models on Chat Data: Using DeepSpeed and LORA for Efficient Training","description":"In this post, I will go through the process of training a large language model on chat data, specifically using the LLaMA-7b model. The fine-tuned model has been shown to perform on par or better than most Hugging Face variants when trained on cleaned alpaca data. I will go into the benefits of using DeepSpeed for training and how LORA (Low-Rank Adaptation) can be used in combination with DeepSpeed to be very efficient during supervised fine-tuning on large models.","keywords":[],"articleBody":"In this post, I will go through the process of training a large language model on chat data, specifically using the LLaMA-7b model. The fine-tuned model has been shown to perform on par or better than most Hugging Face variants when trained on cleaned alpaca data. I will go into the benefits of using DeepSpeed for training and how LORA (Low-Rank Adaptation) can be used in combination with DeepSpeed to be very efficient during supervised fine-tuning on large models.\nThe code can be found at github\nSetting up the Environment First, let’s set up a container for our training environment. Go to the dockercontext folder and run the following commands:\ncd dockercontext make build Then, enter the container:\nmake enter Preparing the Dataset I have used cleaned alpaca data for training the model, but any data in a similar format will work. To set up the dataset, execute the following command:\npython src/setup.py --local --model 'decapoda-research/llama-7b-hf' --data_file data/alpaca_data_cleaned.json --max_tokens 512 If the --local flag is not set, the dataset will be uploaded to Azure ML along with the tokenizer model. The --model argument accepts Hugging Face decoder models.\nDeepSpeed: Why Do We Need It? Training large language models can be computationally expensive, and memory constraints is often a significant issue. DeepSpeed is a library that optimizes the training process and reduces memory requirements.\nDeepSpeed uses “model parallelism” where the model is divided into smaller parts, and each part is trained on a different GPU. This technique helps to scale up the model size without running into memory limitations. In other words, models that would not fit on a single GPU can be partitioned into smaller parts that can be sent out to multiple GPU:s.\nDeepSpeed Stages DeepSpeed stages refer to the different levels of optimization applied during training. These stages help control memory usage and improve training efficiency. The table below demonstrates the memory requirements for different scenarios when training the LLaMA 7 billion model with LORA rank 16:\n   Scenario Model Precision Batch Size DeepSpeed Stage Memory (GB)     Inference Llama 7b 16-bit 1 N/A 13   Inference Llama 7b 8-bit 1 N/A 8   Train Llama 7b 16-bit 2 N/A  49   Train Llama 7b 16-bit 2 2 27   Train Llama 7b 16-bit 4 2 38   Train Llama 7b 16-bit 4 2 43   Train Llama 7b 16-bit 4 3 45   Train Llama 7b 16-bit 6 2  49    Training the Model The LLaMA-7b model was trained using a set of configurations, see config.yaml to achieve a balance between training speed, memory utilization, and model performance. The training process used 16-bit precision, which considerably reduces memory usage and accelerates the training process, compared to 32-bit precision.\nThe training used gradient accumulation over two steps, doubling the effective batch size to 10 without increasing GPU memory requirements. This technique allows for stable training with larger batch sizes even on GPUs with limited memory capacity. A batch size of five was chosen for this training process to ensure efficient use of the available GPU memory.\nThe training was carried out for five epochs, which took approximately 10 hours to complete on two NVIDIA RTX A6000 GPUs (49GB of memory).\nTo train the model, run the following command:\npython src/train_lightning.py --config config.yaml --local The config.yaml file contains all options that control the model training, from DeepSpeed configuration options to model options and flags for performing LORA-training. If the --local flag is set, the finished model will be uploaded to Azure ML.\n Images Example  .image-container { display: flex; } .image-container img { width: 800px; /* Change the width to your desired size */ height: 200px; /* Change the height to your desired size */ } .image-container img:first-child { margin-right: 100px; /* Add a 20px margin to the right of the first image */ }     Loss During Training   Accuracy During Training    The Importance of LORA (Low-Rank Adaptation) LORA is using the insight that weight matrices $W_{i}$ in a pre-trained model contain significant redundancy, in other words the intrinsic dimension of $x$ in the equation $y = W_{i} x$ is much smaller than the embedding dimension.\nTo exploit this, LORA applies low-rank decomposition to these matrices, effectively approximating them using a product of two smaller matrices with lower rank $r$.\n$$ y = W_{i} x + \\alpha (W^L_{b} W^L_{a} x) = (W_{i} + \\alpha + W^L_{b} W^L_{a}) x = W^{*}_{i} x $$\nwhere $W^L_{a} x$ maps x from $[n, 1]$ to $[r, 1]$ (its intrinsic dimension?), where $n » r$. $W^L_{b}$ then expand the the intrinsic mapping back to the original number of dimensions.\nDuring the fine-tuning process, with the task-specific data, LORA only updates the smaller, low-rank matrices, while the original weights are kept frozen - maintaining the model’s expressiveness. This allows the model to adapt to the specific task with fewer trainable parameters and less computational resource. After fine-tuning, the three weight matrices can be combines into $W^{*}_{i}$ resulting in zero extra computational impact compared to a model trained without LORA.\nBy employing LORA during fine-tuning, we can achieve the following benefits:\n Improved training efficiency: LORA significantly reduces the number of parameters to be fine-tuned, which in turn decreases training time and computational resources required. Better generalization: LORA helps the model better adapt to the task-specific data, allowing it to generalize better to new examples. Reduced overfitting: The low-rank adaptation reduces the risk of overfitting to the training data by constraining the model’s capacity.  Training the Model Using the Reacher Library The Reacher library simplifies the process of setting up a remote training environment. First, set up the Reacher instance, connecting to the remote machine running the Docker images in dockercontext:\nfrom reacher.reacher import Reacher, ReacherDocker, RemoteClient from reacher.reacher import create_notebook, create_tensorboard reacher = Reacher( build_name=\"training_alpacha_lora\", host=\"\", port=8961, user=\"root\", ssh_key_filepath=\".ssh/id_rsa\", prefix_cmd=\"PATH=/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home//.local/bin\" ) reacher.setup() You can also create a TensorBoard session on the remote and set up port forwarding between the remote and your local machine:\ncreate_tensorboard(reacher, remote_port=55069, local_port=55069, logdir=reacher.build_path+\"/artifacts/runs\") A notebook server can be set up on the remote, with port forwarding to your local machine:\ncreate_notebook(reacher, remote_port=55068, local_port=55068) To process the data and train the model using the Reacher library, execute the following commands:\n# Process the data reacher.execute( \"python src/setup.py --local --model 'decapoda-research/llama-7b-hf' --data_file alpaca_data_cleaned.json --max_tokens 512\", context=[\"src\", \"data/alpaca_data_cleaned.json\"], wrap_in_screen=True, named_session=\"setup\", ) # Train the model reacher.execute( \"python src/train_lightning.py --config config.yaml --local\", context=[\"src\", \"config.yaml\"], wrap_in_screen=True, named_session=\"training\", ) After training, you can grab the checkpoint:\nreacher.get(\"/final.ckpt\", \"weights/\") Inference For the inference process, refer to the inference.ipynb notebook. Load the configuration used during training and set the 8-bit mode to be true for the inference:\nconf = yaml.safe_load(Path(\"config.yaml\").read_text()) conf[\"model\"][\"load_in_8bit\"] = True conf[\"model\"][\"device_map\"] = \"auto\" Set up the model:\nmodel = GeneratorModel(**conf[\"model\"], torch_dtype=torch.float16) model.load_lora_pretrained(\"weights/alpacha_lora/adapter_model.bin\") Finally, use the trained model to generate text:\nfor text in texts: response = generate_text( model=model, tokenizer=tokenizer, text=text, max_length=150, device=\"cuda\", ) print(response + \"\\n\\n\" + \"-\" * 40 + \"\\n\\n\") Inference Examples In this section, I have collected some examples of the generated responses from the trained model. These examples demonstrate how the model can provide contextually relevant and creative responses to different inputs.\nKeep in mind that the model might produce some hallucinations or inaccuracies, but overall, it demonstrates an understanding of the given context and instructions.\nExample 1: Instruction: Think of a way to use the following material to build a shelter.\nInput: sticks and a piece of cloth\nResponse: You can create a simple lean-to by using sticks as poles and wrapping them around a tree or other structure. The stick will hold up the roof while the cloth will provide protection from rain and wind.\nExample 2: Instruction: How can you stack the objects in a stable manner?\nInput: a book, 9 eggs, a nail, a plant, a shoe\nResponse: The egg should be placed on top of the book and then the shoes could go next to it. The plant would need to be placed at the bottom of the pile. Then the nails could be used as support for the whole structure by placing them underneath the base of the plant. This way they will provide stability without being seen or touching any part of the object.\nExample 3: Instruction: Which is the third largest city in Sweden, list the first and second as well\nResponse: The third largest city in Sweden is Gothenburg (Göteborg). The first two are Stockholm and Malmö.\nExample 4: Instruction: Write a poem describing my love for my wife\nInput: We meet 10 years ago\nResponse: My life has been filled with joy since I met you so long ago I’ve found my soul mate, our bond strong as steel Our hearts are one, forevermore Through thick and thin, through it all The world can never separate us two She’s always there to comfort me And she’ll be by my side till death do us part Love of this kindredominever Above the stars above Brighting brightest star Shining on high Inspiring me to fly higher than ever before Without her I would have no where to go Her heart is mine, and mine hers Foreverlasting No matter what comes or\nExample 5: Instruction: Who is the prime minister in Sweden?\nResponse: The Prime Minister of Sweden is Stefan Löfven, who has been serving since 2014. He was appointed by King Carl XVI Gustaf.\nExample 6: Instruction: Make up a game for two people that likes math and physics, make some examples as well\nResponse: A fun game to play with someone who enjoys math and physics could be called “The Physics Game”. In this game you have to answer questions about physics such as how far away from Earth would you need to travel in order to reach light speed? The first person to get all of their answers correct wins! Another example of a game like this one can be called “Physics Puzzle” where players must solve puzzles based on physics principles. For instance, they are given a picture of a car and then asked what type of engine it has or which way does its wheels move when it turns left or right. This game requires knowledge of basic physics concepts but also problem-solving skills. 2 more games like these include “Physics\nConclusion In conclusion, training large (+7b) language models on chat data with DeepSpeed and LORA enables efficient fine-tuning, reducing memory requirements, and improving generalization. DeepSpeed provides a scalable and memory-efficient solution for training very large-scale models, while LORA helps the model adapt to task-specific data with low-rank adapation of the network’s weights. These techniques, when combined, is very efficient when fine-tuning state-of-the-art language models.\n","wordCount":"1755","inLanguage":"en","datePublished":"2023-04-25T00:00:00Z","dateModified":"2023-04-25T00:00:00Z","author":{"@type":"Person","name":"Johannes Skog"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://johannes-skog.github.io/post/llm-chat/"},"publisher":{"@type":"Organization","name":"Johannes Skog","logo":{"@type":"ImageObject","url":"https://johannes-skog.github.io/%3Clink%20/%20abs%20url%3E"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://johannes-skog.github.io/ accesskey=h title="Home (Alt + H)">
<img src=https://johannes-skog.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a>
<div class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</div>
</div>
<ul id=menu>
<li>
<a href=https://johannes-skog.github.io/page/about/ title="about me">
<span>about me</span>
</a>
</li>
<li>
<a href=https://johannes-skog.github.io/tags/ title=tags>
<span>tags</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://johannes-skog.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://johannes-skog.github.io/post/>Posts</a></div>
<h1 class=post-title>
Training Large (+7b) Language Models on Chat Data: Using DeepSpeed and LORA for Efficient Training
</h1>
<div class=post-meta><span title="2023-04-25 00:00:00 +0000 UTC">April 25, 2023</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;1755 words&nbsp;·&nbsp;Johannes Skog
</div>
</header>
<div class=post-content><p>In this post, I will go through the process of training a large language model on chat data, specifically using the LLaMA-7b model. The fine-tuned model has been shown to perform on par or better than most Hugging Face variants when trained on cleaned alpaca data. I will go into the benefits of using DeepSpeed for training and how LORA (Low-Rank Adaptation) can be used in combination with DeepSpeed to be very efficient during supervised fine-tuning on large models.</p>
<p>The code can be found at <a href=https://github.com/johannes-skog/llm-chat>github</a></p>
<h1 id=setting-up-the-environment>Setting up the Environment<a hidden class=anchor aria-hidden=true href=#setting-up-the-environment>#</a></h1>
<p>First, let&rsquo;s set up a container for our training environment. Go to the dockercontext folder and run the following commands:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=nb>cd</span> dockercontext
make build
</code></pre></div><p>Then, enter the container:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>make enter
</code></pre></div><h1 id=preparing-the-dataset>Preparing the Dataset<a hidden class=anchor aria-hidden=true href=#preparing-the-dataset>#</a></h1>
<p>I have used cleaned alpaca data for training the model, but any data in a similar format will work. To set up the dataset, execute the following command:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=n>python</span> <span class=n>src</span><span class=o>/</span><span class=n>setup</span><span class=o>.</span><span class=n>py</span> <span class=o>--</span><span class=n>local</span> <span class=o>--</span><span class=n>model</span> <span class=s1>&#39;decapoda-research/llama-7b-hf&#39;</span> <span class=o>--</span><span class=n>data_file</span> <span class=n>data</span><span class=o>/</span><span class=n>alpaca_data_cleaned</span><span class=o>.</span><span class=n>json</span> <span class=o>--</span><span class=n>max_tokens</span> <span class=mi>512</span>
</code></pre></div><p>If the <code>--local</code> flag is not set, the dataset will be uploaded to Azure ML along with the tokenizer model. The <code>--model</code> argument accepts Hugging Face decoder models.</p>
<h1 id=deepspeed-why-do-we-need-it>DeepSpeed: Why Do We Need It?<a hidden class=anchor aria-hidden=true href=#deepspeed-why-do-we-need-it>#</a></h1>
<p>Training large language models can be computationally expensive, and memory constraints is often a significant issue. DeepSpeed is a library that optimizes the training process and reduces memory requirements.</p>
<p>DeepSpeed uses &ldquo;model parallelism&rdquo; where the model is divided into smaller parts, and each part is trained on a different GPU. This technique helps to scale up the model size without running into memory limitations. In other words, models that would not fit on a single GPU can be partitioned into smaller parts that can be sent out to multiple GPU:s.</p>
<h2 id=deepspeed-stages>DeepSpeed Stages<a hidden class=anchor aria-hidden=true href=#deepspeed-stages>#</a></h2>
<p>DeepSpeed stages refer to the different levels of optimization applied during training. These stages help control memory usage and improve training efficiency. The table below demonstrates the memory requirements for different scenarios when training the LLaMA 7 billion model with LORA rank 16:</p>
<table>
<thead>
<tr>
<th>Scenario</th>
<th>Model</th>
<th>Precision</th>
<th>Batch Size</th>
<th>DeepSpeed Stage</th>
<th>Memory (GB)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Inference</td>
<td>Llama 7b</td>
<td>16-bit</td>
<td>1</td>
<td>N/A</td>
<td>13</td>
</tr>
<tr>
<td>Inference</td>
<td>Llama 7b</td>
<td>8-bit</td>
<td>1</td>
<td>N/A</td>
<td>8</td>
</tr>
<tr>
<td>Train</td>
<td>Llama 7b</td>
<td>16-bit</td>
<td>2</td>
<td>N/A</td>
<td>> 49</td>
</tr>
<tr>
<td>Train</td>
<td>Llama 7b</td>
<td>16-bit</td>
<td>2</td>
<td>2</td>
<td>27</td>
</tr>
<tr>
<td>Train</td>
<td>Llama 7b</td>
<td>16-bit</td>
<td>4</td>
<td>2</td>
<td>38</td>
</tr>
<tr>
<td>Train</td>
<td>Llama 7b</td>
<td>16-bit</td>
<td>4</td>
<td>2</td>
<td>43</td>
</tr>
<tr>
<td>Train</td>
<td>Llama 7b</td>
<td>16-bit</td>
<td>4</td>
<td>3</td>
<td>45</td>
</tr>
<tr>
<td>Train</td>
<td>Llama 7b</td>
<td>16-bit</td>
<td>6</td>
<td>2</td>
<td>> 49</td>
</tr>
</tbody>
</table>
<h1 id=training-the-model>Training the Model<a hidden class=anchor aria-hidden=true href=#training-the-model>#</a></h1>
<p>The LLaMA-7b model was trained using a set of configurations, see <code>config.yaml</code> to achieve a balance between training speed, memory utilization, and model performance. The training process used 16-bit precision, which considerably reduces memory usage and accelerates the training process, compared to 32-bit precision.</p>
<p>The training used gradient accumulation over two steps, doubling the effective batch size to 10 without increasing GPU memory requirements. This technique allows for stable training with larger batch sizes even on GPUs with limited memory capacity. A batch size of five was chosen for this training process to ensure efficient use of the available GPU memory.</p>
<p>The training was carried out for five epochs, which took approximately 10 hours to complete on two NVIDIA RTX A6000 GPUs (49GB of memory).</p>
<p>To train the model, run the following command:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=n>python</span> <span class=n>src</span><span class=o>/</span><span class=n>train_lightning</span><span class=o>.</span><span class=n>py</span> <span class=o>--</span><span class=n>config</span> <span class=n>config</span><span class=o>.</span><span class=n>yaml</span> <span class=o>--</span><span class=n>local</span>
</code></pre></div><p>The config.yaml file contains all options that control the model training, from DeepSpeed configuration options to model options and flags for performing LORA-training. If the <code>--local</code> flag is set, the finished model will be uploaded to Azure ML.</p>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=.5">
<title>Images Example</title>
<style>.image-container{display:flex}.image-container img{width:800px;height:200px}.image-container img:first-child{margin-right:100px}</style>
</head>
<body>
<div class=image-container>
<figure>
<img src=loss.png alt=Loss>
<figcaption>Loss During Training</figcaption>
</figure>
<figure>
<img src=accuracy.png alt=Accuracy>
<figcaption>Accuracy During Training</figcaption>
</figure>
</div>
</body>
<h1 id=the-importance-of-lora-low-rank-adaptation>The Importance of LORA (Low-Rank Adaptation)<a hidden class=anchor aria-hidden=true href=#the-importance-of-lora-low-rank-adaptation>#</a></h1>
<p>LORA is using the insight that weight matrices $W_{i}$ in a pre-trained model contain significant redundancy, in other words the intrinsic dimension of $x$ in the equation $y = W_{i} x$ is much smaller than the embedding dimension.</p>
<p>To exploit this, LORA applies low-rank decomposition to these matrices, effectively approximating them using a product of two smaller matrices with lower rank $r$.</p>
<p>$$
y = W_{i} x + \alpha (W^L_{b} W^L_{a} x) = (W_{i} + \alpha + W^L_{b} W^L_{a}) x = W^{*}_{i} x
$$</p>
<p>where $W^L_{a} x$ maps x from $[n, 1]$ to $[r, 1]$ (its intrinsic dimension?), where $n &#187; r$. $W^L_{b}$ then expand the the intrinsic mapping back to the original number of dimensions.</p>
<p>During the fine-tuning process, with the task-specific data, LORA only updates the smaller, low-rank matrices, while the original weights are kept frozen - maintaining the model&rsquo;s expressiveness. This allows the model to adapt to the specific task with fewer trainable parameters and less computational resource. After fine-tuning, the three weight matrices can be combines into $W^{*}_{i}$ resulting in zero extra computational impact compared to a model trained without LORA.</p>
<p>By employing LORA during fine-tuning, we can achieve the following benefits:</p>
<ul>
<li>Improved training efficiency: LORA significantly reduces the number of parameters to be fine-tuned, which in turn decreases training time and computational resources required.</li>
<li>Better generalization: LORA helps the model better adapt to the task-specific data, allowing it to generalize better to new examples.</li>
<li>Reduced overfitting: The low-rank adaptation reduces the risk of overfitting to the training data by constraining the model&rsquo;s capacity.</li>
</ul>
<h1 id=training-the-model-using-the-reacher-library>Training the Model Using the Reacher Library<a hidden class=anchor aria-hidden=true href=#training-the-model-using-the-reacher-library>#</a></h1>
<p>The Reacher library simplifies the process of setting up a remote training environment. First, set up the Reacher instance, connecting to the remote machine running the Docker images in dockercontext:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=kn>from</span> <span class=nn>reacher.reacher</span> <span class=kn>import</span> <span class=n>Reacher</span><span class=p>,</span> <span class=n>ReacherDocker</span><span class=p>,</span> <span class=n>RemoteClient</span>
<span class=kn>from</span> <span class=nn>reacher.reacher</span> <span class=kn>import</span> <span class=n>create_notebook</span><span class=p>,</span> <span class=n>create_tensorboard</span>

<span class=n>reacher</span> <span class=o>=</span> <span class=n>Reacher</span><span class=p>(</span>
    <span class=n>build_name</span><span class=o>=</span><span class=s2>&#34;training_alpacha_lora&#34;</span><span class=p>,</span>
    <span class=n>host</span><span class=o>=</span><span class=s2>&#34;&#34;</span><span class=p>,</span>
    <span class=n>port</span><span class=o>=</span><span class=mi>8961</span><span class=p>,</span>
    <span class=n>user</span><span class=o>=</span><span class=s2>&#34;root&#34;</span><span class=p>,</span>
    <span class=n>ssh_key_filepath</span><span class=o>=</span><span class=s2>&#34;.ssh/id_rsa&#34;</span><span class=p>,</span>
    <span class=n>prefix_cmd</span><span class=o>=</span><span class=s2>&#34;PATH=/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home//.local/bin&#34;</span>
<span class=p>)</span>
<span class=n>reacher</span><span class=o>.</span><span class=n>setup</span><span class=p>()</span>
</code></pre></div><p>You can also create a TensorBoard session on the remote and set up port forwarding between the remote and your local machine:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=n>create_tensorboard</span><span class=p>(</span><span class=n>reacher</span><span class=p>,</span> <span class=n>remote_port</span><span class=o>=</span><span class=mi>55069</span><span class=p>,</span> <span class=n>local_port</span><span class=o>=</span><span class=mi>55069</span><span class=p>,</span> <span class=n>logdir</span><span class=o>=</span><span class=n>reacher</span><span class=o>.</span><span class=n>build_path</span><span class=o>+</span><span class=s2>&#34;/artifacts/runs&#34;</span><span class=p>)</span>
</code></pre></div><p>A notebook server can be set up on the remote, with port forwarding to your local machine:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=n>create_notebook</span><span class=p>(</span><span class=n>reacher</span><span class=p>,</span> <span class=n>remote_port</span><span class=o>=</span><span class=mi>55068</span><span class=p>,</span> <span class=n>local_port</span><span class=o>=</span><span class=mi>55068</span><span class=p>)</span>
</code></pre></div><p>To process the data and train the model using the Reacher library, execute the following commands:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=c1># Process the data</span>
<span class=n>reacher</span><span class=o>.</span><span class=n>execute</span><span class=p>(</span>
    <span class=s2>&#34;python src/setup.py --local --model &#39;decapoda-research/llama-7b-hf&#39; --data_file alpaca_data_cleaned.json --max_tokens 512&#34;</span><span class=p>,</span>
    <span class=n>context</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;src&#34;</span><span class=p>,</span> <span class=s2>&#34;data/alpaca_data_cleaned.json&#34;</span><span class=p>],</span>
    <span class=n>wrap_in_screen</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
    <span class=n>named_session</span><span class=o>=</span><span class=s2>&#34;setup&#34;</span><span class=p>,</span>
<span class=p>)</span>
<span class=c1># Train the model</span>
<span class=n>reacher</span><span class=o>.</span><span class=n>execute</span><span class=p>(</span>
    <span class=s2>&#34;python src/train_lightning.py --config config.yaml --local&#34;</span><span class=p>,</span>
    <span class=n>context</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;src&#34;</span><span class=p>,</span> <span class=s2>&#34;config.yaml&#34;</span><span class=p>],</span>
    <span class=n>wrap_in_screen</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
    <span class=n>named_session</span><span class=o>=</span><span class=s2>&#34;training&#34;</span><span class=p>,</span>
<span class=p>)</span>
</code></pre></div><p>After training, you can grab the checkpoint:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=n>reacher</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;&lt;path&gt;/final.ckpt&#34;</span><span class=p>,</span> <span class=s2>&#34;weights/&#34;</span><span class=p>)</span>
</code></pre></div><h1 id=inference>Inference<a hidden class=anchor aria-hidden=true href=#inference>#</a></h1>
<p>For the inference process, refer to the <code>inference.ipynb</code> notebook. Load the configuration used during training and set the 8-bit mode to be true for the inference:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=n>conf</span> <span class=o>=</span> <span class=n>yaml</span><span class=o>.</span><span class=n>safe_load</span><span class=p>(</span><span class=n>Path</span><span class=p>(</span><span class=s2>&#34;config.yaml&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>read_text</span><span class=p>())</span>
<span class=n>conf</span><span class=p>[</span><span class=s2>&#34;model&#34;</span><span class=p>][</span><span class=s2>&#34;load_in_8bit&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=kc>True</span>
<span class=n>conf</span><span class=p>[</span><span class=s2>&#34;model&#34;</span><span class=p>][</span><span class=s2>&#34;device_map&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=s2>&#34;auto&#34;</span>
</code></pre></div><p>Set up the model:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=n>model</span> <span class=o>=</span> <span class=n>GeneratorModel</span><span class=p>(</span><span class=o>**</span><span class=n>conf</span><span class=p>[</span><span class=s2>&#34;model&#34;</span><span class=p>],</span> <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float16</span><span class=p>)</span>
<span class=n>model</span><span class=o>.</span><span class=n>load_lora_pretrained</span><span class=p>(</span><span class=s2>&#34;weights/alpacha_lora/adapter_model.bin&#34;</span><span class=p>)</span>
</code></pre></div><p>Finally, use the trained model to generate text:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>for</span> <span class=n>text</span> <span class=ow>in</span> <span class=n>texts</span><span class=p>:</span>
    <span class=n>response</span> <span class=o>=</span> <span class=n>generate_text</span><span class=p>(</span>
        <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span>
        <span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span>
        <span class=n>text</span><span class=o>=</span><span class=n>text</span><span class=p>,</span>
        <span class=n>max_length</span><span class=o>=</span><span class=mi>150</span><span class=p>,</span>
        <span class=n>device</span><span class=o>=</span><span class=s2>&#34;cuda&#34;</span><span class=p>,</span>
    <span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=n>response</span> <span class=o>+</span> <span class=s2>&#34;</span><span class=se>\n\n</span><span class=s2>&#34;</span> <span class=o>+</span> <span class=s2>&#34;-&#34;</span> <span class=o>*</span> <span class=mi>40</span> <span class=o>+</span> <span class=s2>&#34;</span><span class=se>\n\n</span><span class=s2>&#34;</span><span class=p>)</span>
</code></pre></div><h2 id=inference-examples>Inference Examples<a hidden class=anchor aria-hidden=true href=#inference-examples>#</a></h2>
<p>In this section, I have collected some examples of the generated responses from the trained model. These examples demonstrate how the model can provide contextually relevant and creative responses to different inputs.</p>
<p>Keep in mind that the model might produce some hallucinations or inaccuracies, but overall, it demonstrates an understanding of the given context and instructions.</p>
<h3 id=example-1>Example 1:<a hidden class=anchor aria-hidden=true href=#example-1>#</a></h3>
<p>Instruction: Think of a way to use the following material to build a shelter.</p>
<p>Input: sticks and a piece of cloth</p>
<p>Response: You can create a simple lean-to by using sticks as poles and wrapping them around a tree or other structure. The stick will hold up the roof while the cloth will provide protection from rain and wind.</p>
<h3 id=example-2>Example 2:<a hidden class=anchor aria-hidden=true href=#example-2>#</a></h3>
<p>Instruction: How can you stack the objects in a stable manner?</p>
<p>Input: a book, 9 eggs, a nail, a plant, a shoe</p>
<p>Response: The egg should be placed on top of the book and then the shoes could go next to it. The plant would need to be placed at the bottom of the pile. Then the nails could be used as support for the whole structure by placing them underneath the base of the plant. This way they will provide stability without being seen or touching any part of the object.</p>
<h3 id=example-3>Example 3:<a hidden class=anchor aria-hidden=true href=#example-3>#</a></h3>
<p>Instruction: Which is the third largest city in Sweden, list the first and second as well</p>
<p>Response: The third largest city in Sweden is Gothenburg (Göteborg). The first two are Stockholm and Malmö.</p>
<h3 id=example-4>Example 4:<a hidden class=anchor aria-hidden=true href=#example-4>#</a></h3>
<p>Instruction: Write a poem describing my love for my wife</p>
<p>Input: We meet 10 years ago</p>
<p>Response: My life has been filled with joy since I met you so long ago
I&rsquo;ve found my soul mate, our bond strong as steel
Our hearts are one, forevermore
Through thick and thin, through it all
The world can never separate us two
She&rsquo;s always there to comfort me
And she&rsquo;ll be by my side till death do us part
Love of this kindredominever
Above the stars above
Brighting brightest star
Shining on high
Inspiring me to fly higher than ever before
Without her I would have no where to go
Her heart is mine, and mine hers
Foreverlasting
No matter what comes or</p>
<h3 id=example-5>Example 5:<a hidden class=anchor aria-hidden=true href=#example-5>#</a></h3>
<p>Instruction: Who is the prime minister in Sweden?</p>
<p>Response: The Prime Minister of Sweden is Stefan Löfven, who has been serving since 2014. He was appointed by King Carl XVI Gustaf.</p>
<h3 id=example-6>Example 6:<a hidden class=anchor aria-hidden=true href=#example-6>#</a></h3>
<p>Instruction: Make up a game for two people that likes math and physics, make some examples as well</p>
<p>Response: A fun game to play with someone who enjoys math and physics could be called &ldquo;The Physics Game&rdquo;. In this game you have to answer questions about physics such as how far away from Earth would you need to travel in order to reach light speed? The first person to get all of their answers correct wins! Another example of a game like this one can be called “Physics Puzzle” where players must solve puzzles based on physics principles. For instance, they are given a picture of a car and then asked what type of engine it has or which way does its wheels move when it turns left or right. This game requires knowledge of basic physics concepts but also problem-solving skills. 2 more games like these include “Physics</p>
<h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1>
<p>In conclusion, training large (+7b) language models on chat data with DeepSpeed and LORA enables efficient fine-tuning, reducing memory requirements, and improving generalization. DeepSpeed provides a scalable and memory-efficient solution for training very large-scale models, while LORA helps the model adapt to task-specific data with low-rank adapation of the network&rsquo;s weights. These techniques, when combined, is very efficient when fine-tuning state-of-the-art language models.</p>
</div>
<footer class=post-footer>
<ul class=post-tags>
</ul>
<nav class=paginav>
<a class=next href=https://johannes-skog.github.io/post/sentiment-deployment/>
<span class=title>Next »</span>
<br>
<span>Training & Deploying LLMs: A Step-by-Step Guide</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Training Large (+7b) Language Models on Chat Data: Using DeepSpeed and LORA for Efficient Training on twitter" href="https://twitter.com/intent/tweet/?text=Training%20Large%20%28%2b7b%29%20Language%20Models%20on%20Chat%20Data%3a%20Using%20DeepSpeed%20and%20LORA%20for%20Efficient%20Training&url=https%3a%2f%2fjohannes-skog.github.io%2fpost%2fllm-chat%2f&hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Training Large (+7b) Language Models on Chat Data: Using DeepSpeed and LORA for Efficient Training on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fjohannes-skog.github.io%2fpost%2fllm-chat%2f&title=Training%20Large%20%28%2b7b%29%20Language%20Models%20on%20Chat%20Data%3a%20Using%20DeepSpeed%20and%20LORA%20for%20Efficient%20Training&summary=Training%20Large%20%28%2b7b%29%20Language%20Models%20on%20Chat%20Data%3a%20Using%20DeepSpeed%20and%20LORA%20for%20Efficient%20Training&source=https%3a%2f%2fjohannes-skog.github.io%2fpost%2fllm-chat%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Training Large (+7b) Language Models on Chat Data: Using DeepSpeed and LORA for Efficient Training on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fjohannes-skog.github.io%2fpost%2fllm-chat%2f&title=Training%20Large%20%28%2b7b%29%20Language%20Models%20on%20Chat%20Data%3a%20Using%20DeepSpeed%20and%20LORA%20for%20Efficient%20Training"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Training Large (+7b) Language Models on Chat Data: Using DeepSpeed and LORA for Efficient Training on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fjohannes-skog.github.io%2fpost%2fllm-chat%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Training Large (+7b) Language Models on Chat Data: Using DeepSpeed and LORA for Efficient Training on whatsapp" href="https://api.whatsapp.com/send?text=Training%20Large%20%28%2b7b%29%20Language%20Models%20on%20Chat%20Data%3a%20Using%20DeepSpeed%20and%20LORA%20for%20Efficient%20Training%20-%20https%3a%2f%2fjohannes-skog.github.io%2fpost%2fllm-chat%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Training Large (+7b) Language Models on Chat Data: Using DeepSpeed and LORA for Efficient Training on telegram" href="https://telegram.me/share/url?text=Training%20Large%20%28%2b7b%29%20Language%20Models%20on%20Chat%20Data%3a%20Using%20DeepSpeed%20and%20LORA%20for%20Efficient%20Training&url=https%3a%2f%2fjohannes-skog.github.io%2fpost%2fllm-chat%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2023 <a href=https://johannes-skog.github.io/>Johannes Skog</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerHTML='copy';function d(){a.innerHTML='copied!',setTimeout(()=>{a.innerHTML='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>