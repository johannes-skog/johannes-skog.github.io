<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Training & Deploying LLMs: A Step-by-Step Guide | Johannes Skog</title>
<meta name=keywords content>
<meta name=description content="In this post, I will go through all the necessary steps to set up and train a state-of-the-art LLM for sentiment analysis (and many other NLP applications since the steps are almost the same) on Twitter data. I will cover the entire pipeline, from creating the training dataset to deploying the model using TorchServe and Kubernetes on Azure.
See github for the code
Setting up the Training Pipeline Setting Up the Environment First, we need to set up our environment to run the training and deployment steps.">
<meta name=author content="Johannes Skog">
<link rel=canonical href=https://johannes-skog.github.io/post/sentiment-deployment/>
<meta name=google-site-verification content="XYZabc">
<meta name=yandex-verification content="XYZabc">
<meta name=msvalidate.01 content="XYZabc">
<link crossorigin=anonymous href=/assets/css/stylesheet.2b2689f619f5ca0e2da45710b8e56846446fb899d5e011fa5ccb6a8131af1349.css integrity="sha256-KyaJ9hn1yg4tpFcQuOVoRkRvuJnV4BH6XMtqgTGvE0k=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://johannes-skog.github.io/%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=16x16 href=https://johannes-skog.github.io/%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=32x32 href=https://johannes-skog.github.io/%3Clink%20/%20abs%20url%3E>
<link rel=apple-touch-icon href=https://johannes-skog.github.io/%3Clink%20/%20abs%20url%3E>
<link rel=mask-icon href=https://johannes-skog.github.io/%3Clink%20/%20abs%20url%3E>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script type=text/javascript>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-123-45','auto'),ga('send','pageview'))</script><meta property="og:title" content="Training & Deploying LLMs: A Step-by-Step Guide">
<meta property="og:description" content="In this post, I will go through all the necessary steps to set up and train a state-of-the-art LLM for sentiment analysis (and many other NLP applications since the steps are almost the same) on Twitter data. I will cover the entire pipeline, from creating the training dataset to deploying the model using TorchServe and Kubernetes on Azure.
See github for the code
Setting up the Training Pipeline Setting Up the Environment First, we need to set up our environment to run the training and deployment steps.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://johannes-skog.github.io/post/sentiment-deployment/"><meta property="og:image" content="https://johannes-skog.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="post">
<meta property="article:published_time" content="2023-04-18T00:00:00+00:00">
<meta property="article:modified_time" content="2023-04-18T00:00:00+00:00"><meta property="og:site_name" content="Johannes Skog">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://johannes-skog.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E">
<meta name=twitter:title content="Training & Deploying LLMs: A Step-by-Step Guide">
<meta name=twitter:description content="In this post, I will go through all the necessary steps to set up and train a state-of-the-art LLM for sentiment analysis (and many other NLP applications since the steps are almost the same) on Twitter data. I will cover the entire pipeline, from creating the training dataset to deploying the model using TorchServe and Kubernetes on Azure.
See github for the code
Setting up the Training Pipeline Setting Up the Environment First, we need to set up our environment to run the training and deployment steps.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://johannes-skog.github.io/post/"},{"@type":"ListItem","position":2,"name":"Training \u0026 Deploying LLMs: A Step-by-Step Guide","item":"https://johannes-skog.github.io/post/sentiment-deployment/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Training \u0026 Deploying LLMs: A Step-by-Step Guide","name":"Training \u0026 Deploying LLMs: A Step-by-Step Guide","description":"In this post, I will go through all the necessary steps to set up and train a state-of-the-art LLM for sentiment analysis (and many other NLP applications since the steps are almost the same) on Twitter data. I will cover the entire pipeline, from creating the training dataset to deploying the model using TorchServe and Kubernetes on Azure.\nSee github for the code\nSetting up the Training Pipeline Setting Up the Environment First, we need to set up our environment to run the training and deployment steps.","keywords":[],"articleBody":"In this post, I will go through all the necessary steps to set up and train a state-of-the-art LLM for sentiment analysis (and many other NLP applications since the steps are almost the same) on Twitter data. I will cover the entire pipeline, from creating the training dataset to deploying the model using TorchServe and Kubernetes on Azure.\nSee github for the code\nSetting up the Training Pipeline Setting Up the Environment First, we need to set up our environment to run the training and deployment steps. I’m using Docker to create a container with all the necessary dependencies. To build and enter the Docker container, execute the following commands:\ncd dockercontext make build make enter Creating the Training Dataset Next, we will create the training data using the following command:\npython src/setup.py --dataset_name twitter-sentiment During this step we are downloading the raw data, thereafter it will get tokenized using the tokenizer associated with the model and finally upload it to Azure ML Datasets for later use.\nTraining the Model Now, it’s time to train the model on the dataset we just created. To do this, run the following command:\npython train.py --dataset twitter-sentiment --lock_embedding --lock_first_n_layers 7 --batch_size 25 --iterations 50000 I also provide an alternative training script that uses PyTorch Lightning with Low-Rank Adaptation (LORA) and a cosine learning rate scheduler:\npython src/train_lightning.py --config config.yaml After the training is completed we will use torch.jit compiler to trace the trained model, during this steps, all computational steps are recorded and optimized and organized into a single file. The traced model can later be used stand-alone without access to the python code that setup the model in the first place.\nBoth the tokenizer, python model and the traced model will be uploaded to Azure ML Models to be used during next steps.\nLow-Rank Adaptation - LORA LORA is using the insight that weight matrices $W_{i}$ in a pre-trained model contain significant redundancy, in other words the intrinsic dimension of $x$ in the equation $y = W_{i} x$ is much smaller than the embedding dimension.\nTo exploit this, LORA applies low-rank decomposition to these matrices, effectively approximating them using a product of two smaller matrices with lower rank $r$.\n$$ y = W_{i} x + \\alpha (W^L_{b} W^L_{a} x) = (W_{i} + \\alpha + W^L_{b} W^L_{a}) x = W^{*}_{i} x $$\nwhere $W^L_{a} x$ maps x from $[n, 1]$ to $[r, 1]$ (its intrinsic dimension?), where $n » r$. $W^L_{b}$ then expand the the intrinsic mapping back to the original number of dimensions.\nDuring the fine-tuning process, LORA only updates the smaller, low-rank matrices, while the original weights are kept frozen. This allows the model to adapt to the specific task with fewer trainable parameters and less computational resource. After fine-tuning, the three weight matrices can be combines into $W^{*}_{i}$ resulting in zero extra computational impact compared to a model trained without LORA.\nPackaging the Model for TorchServe Once the training is complete, we package the model and its dependencies into a .mar file for TorchServe, using the following command:\npython package.py --model_name xlmr_sentiment_traced --version 1 --handler src/handler.py --requirements dockercontext_torchserve/requirements.txt The package script will download the tokenizer and the traced model.\nOnes all files are downloaded we will create the .mar file using the following command:\ntorch-model-archiver \\  --force\\  --model-name sentiment\\  --version {args.version}\\  --serialized-file {DESTINATION_FOLDER}/traced.pt\\  --handler {args.handler}\\  --export-path {DESTINATION_FOLDER}\\  --requirements-file {args.requirements}\\  --extra-files {DESTINATION_FOLDER}/special_tokens_map.json,{DESTINATION_FOLDER}/tokenizer_config.json,{DESTINATION_FOLDER}/tokenizer.json Here we package the traced model, tokenizer, all dependencies specified in the requirements-file and the handler. The handler is responsible for initialize the model and tokenizer and to setup the function that the API will call when there is a request. The handler implements the functions below:\nclass SentimentHandler(BaseHandler): def __init__(self): super().__init__() def initialize(self, context): # Load the model and tokenizer pass def postprocess(self, data): # Convert the output tensor to a list of predictions pass def preprocess(self, requests: List[Dict[str, bytearray]]): # Process the input text and tokenize it pass def inference(self, model_input): # Perform inference using the model and return predictions pass Running the Pipeline Remotely with Reacher To run the pipeline on a remote machine (needed for training - if you are working on a laptop), I’m using the Reacher library. First, set up a Reacher instance with your remote configuration:\nfrom reacher.reacher import RemoteClient, ReacherDocker from dotenv import dotenv_values config = dotenv_values() reacher = ReacherDocker( build_name=\"pytorch_base\", image_name=\"pytorch_base\", build_context=\"dockercontext\", host=config[\"HOST\"], user=config[\"USER\"], password=config[\"PASSWORD\"], ssh_key_filepath=config[\"SSH_KEY_PATH\"], ) reacher.build() reacher.setup( ports=[8888, 6666], envs=dotenv_values(\".env\") ) Then, execute the different pipeline steps on the remote machine:\nreacher.execute( context=[\"src\"], command=\"python .py ... \", named_session=\"\", ) Here you will use the same commands as running it locally, just specify the file that are needed to run the scripts and those will get uploaded to the remote before the commands is executed.\nOnes all steps are done, fetch the .mar file containing the trained model:\nreacher.get(\"artifacts/sentiment.mar\", \"artifacts\") Deployment on Azure Kubernetes Cluster with TorchServe Setting up the Kubernetes cluster on Azure To set up the Kubernetes cluster on Azure, export environment variables from the .env file:\nexport $(cat .env | xargs) Create the cluster:\naz aks create --resource-group ${AZURE_RESOURCE_GROUP} --name ${COMPUTE_CLUSTER_NAME} --node-vm-size Standard_DC2s_v2 --node-count 1 --generate-ssh-keys Here you can specify a more performant node-size than Standard_DC2s_v2.\nInstall the necessary tools, such as kubectl:\naz aks install-cli Set up the credentials and push them to ~/.kube/config so that kubectl can be used:\naz aks get-credentials --resource-group ${AZURE_RESOURCE_GROUP} --name ${COMPUTE_CLUSTER_NAME} Configuring the cluster If you are using GPU-powered nodes we need to setup nvidia-device-plugin to allow the nodes to access the GPUs.\nkubectl apply -f k8s/nvidia-device-plugin-ds.yaml Configuring storage Create a storage class for files across the nodes:\nkubectl apply -f k8s/Azure_file_sc.yaml Create a PersistentVolume to store mar-files and configs:\nkubectl apply -f k8s/AKS_pv_claim.yaml Create a pod with PersistentVolume for copying mar-files and config files:\nkubectl apply -f k8s/model_store_pod.yaml Check the running pods:\nkubectl get pods -o wide Uploading mar/config files to Kubernetes Create a folder on the pod with PersistentVolume to upload mar/config files:\nkubectl exec --tty pod/model-store-pod -- mkdir /mnt/azure/model-store/ Copy the mar file to the pod with PersistentVolume:\nkubectl cp dockercontext_torchserve/model_store/sentiment.mar model-store-pod:/mnt/azure/model-store/sentiment.mar Copy the config files:\nkubectl exec --tty pod/model-store-pod -- mkdir /mnt/azure/config/ kubectl cp dockercontext_torchserve/config.properties model-store-pod:/mnt/azure/config/config.properties Check if all the files have been uploaded:\nkubectl exec --tty pod/model-store-pod -- find /mnt/azure/ Deployment for public access Set up a TorchServe deployment and a service that forwards the inference/management/metric ports to the load balancer for external access:\nkubectl create -f k8s/torchserve_public.yaml Get information about the TorchServe service:\nkubectl describe service torchserve To get information about all services:\nkubectl get service -A Use the external IP along with the correct port to start the sentiment model on the deployment:\ncurl -v -X POST \"http://:/models?initial_workers=1\u0026batch_size=5\u0026maxWorkers=5\u0026max_batch_delay=1000\u0026synchronous=true\u0026url=sentiment.mar\" Here, we set up one initial worker and allow TorchServe to scale up to 5, if needed. We allow a maximum batch size of 5, and if requests arrive within 1000ms, TorchServe groups them in batches of the maximum size.\nTo delete the deployment and service:\nkubectl delete deployment torchserve kubectl delete service torchserve Deploying the Model for Private Access To deploy the model for private access, create a pod with the TorchServe image:\nkubectl create -f k8s/torchserve_private.yaml Access the TorchServe pod:\nkubectl exec --stdin --tty torchserve -- /bin/bash Now, you can set up and test TorchServe in a similar way as for the public deployment. Change  to 127.0.0.1 and use the ports specified in config.properties.\nInference on the Model Finally, you can use your deployed model to perform sentiment analysis on text inputs:\nCopy code curl -X POST -H \"Content-Type: application/json\" -d '[\"The movie was so good\", \"The movie was so bad\"]' http://:/v1/models/sentiment:predict The output will look like this:\n[ 0.9991590976715088, 0.0054536801762878895 ] The model is confident that the first text has a positive sentiment, while the second text has a negative sentiment.\nCleaning Up Resources To delete the deployment and service, run the following commands:\nkubectl delete deployment torchserve kubectl delete service torchserve Conclusion In this post, I have walked through the entire process of training a LLM for sentiment analysis, packaging it for TorchServe, and deploying it on an Azure Kubernetes cluster. With this, you can now create, train, and deploy your own models for various use cases.\n","wordCount":"1358","inLanguage":"en","datePublished":"2023-04-18T00:00:00Z","dateModified":"2023-04-18T00:00:00Z","author":{"@type":"Person","name":"Johannes Skog"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://johannes-skog.github.io/post/sentiment-deployment/"},"publisher":{"@type":"Organization","name":"Johannes Skog","logo":{"@type":"ImageObject","url":"https://johannes-skog.github.io/%3Clink%20/%20abs%20url%3E"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://johannes-skog.github.io/ accesskey=h title="Home (Alt + H)">
<img src=https://johannes-skog.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a>
<div class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</div>
</div>
<ul id=menu>
<li>
<a href=https://johannes-skog.github.io/page/about/ title="about me">
<span>about me</span>
</a>
</li>
<li>
<a href=https://johannes-skog.github.io/tags/ title=tags>
<span>tags</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://johannes-skog.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://johannes-skog.github.io/post/>Posts</a></div>
<h1 class=post-title>
Training & Deploying LLMs: A Step-by-Step Guide
</h1>
<div class=post-meta><span title="2023-04-18 00:00:00 +0000 UTC">April 18, 2023</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1358 words&nbsp;·&nbsp;Johannes Skog
</div>
</header>
<div class=post-content><p>In this post, I will go through all the necessary steps to set up and train a state-of-the-art LLM for sentiment analysis (and many other NLP applications since the steps are almost the same) on Twitter data. I will cover the entire pipeline, from creating the training dataset to deploying the model using TorchServe and Kubernetes on Azure.</p>
<p>See <a href=https://github.com/johannes-skog/sentiment>github</a> for the code</p>
<h1 id=setting-up-the-training-pipeline>Setting up the Training Pipeline<a hidden class=anchor aria-hidden=true href=#setting-up-the-training-pipeline>#</a></h1>
<h2 id=setting-up-the-environment>Setting Up the Environment<a hidden class=anchor aria-hidden=true href=#setting-up-the-environment>#</a></h2>
<p>First, we need to set up our environment to run the training and deployment steps. I&rsquo;m using Docker to create a container with all the necessary dependencies. To build and enter the Docker container, execute the following commands:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=nb>cd</span> dockercontext
make build 
make enter
</code></pre></div><h2 id=creating-the-training-dataset>Creating the Training Dataset<a hidden class=anchor aria-hidden=true href=#creating-the-training-dataset>#</a></h2>
<p>Next, we will create the training data using the following command:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>python src/setup.py --dataset_name twitter-sentiment
</code></pre></div><p>During this step we are downloading the raw data, thereafter it will get tokenized using the tokenizer associated with the model and finally upload it to Azure ML Datasets for later use.</p>
<h2 id=training-the-model>Training the Model<a hidden class=anchor aria-hidden=true href=#training-the-model>#</a></h2>
<p>Now, it&rsquo;s time to train the model on the dataset we just created. To do this, run the following command:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>python train.py --dataset twitter-sentiment --lock_embedding --lock_first_n_layers <span class=m>7</span> --batch_size <span class=m>25</span> --iterations <span class=m>50000</span>
</code></pre></div><p>I also provide an alternative training script that uses PyTorch Lightning with Low-Rank Adaptation (LORA) and a cosine learning rate scheduler:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>python src/train_lightning.py --config config.yaml
</code></pre></div><p>After the training is completed we will use torch.jit compiler to trace the trained model, during this steps, all computational steps are recorded and optimized and organized into a single file. The traced model can later be used stand-alone without access to the python code that setup the model in the first place.</p>
<p>Both the tokenizer, python model and the traced model will be uploaded to Azure ML Models to be used during next steps.</p>
<h3 id=low-rank-adaptation---lora>Low-Rank Adaptation - LORA<a hidden class=anchor aria-hidden=true href=#low-rank-adaptation---lora>#</a></h3>
<p>LORA is using the insight that weight matrices $W_{i}$ in a pre-trained model contain significant redundancy, in other words the intrinsic dimension of $x$ in the equation $y = W_{i} x$ is much smaller than the embedding dimension.</p>
<p>To exploit this, LORA applies low-rank decomposition to these matrices, effectively approximating them using a product of two smaller matrices with lower rank $r$.</p>
<p>$$
y = W_{i} x + \alpha (W^L_{b} W^L_{a} x) = (W_{i} + \alpha + W^L_{b} W^L_{a}) x = W^{*}_{i} x
$$</p>
<p>where $W^L_{a} x$ maps x from $[n, 1]$ to $[r, 1]$ (its intrinsic dimension?), where $n &#187; r$. $W^L_{b}$ then expand the the intrinsic mapping back to the original number of dimensions.</p>
<p>During the fine-tuning process, LORA only updates the smaller, low-rank matrices, while the original weights are kept frozen. This allows the model to adapt to the specific task with fewer trainable parameters and less computational resource. After fine-tuning, the three weight matrices can be combines into $W^{*}_{i}$ resulting in zero extra computational impact compared to a model trained without LORA.</p>
<h2 id=packaging-the-model-for-torchserve>Packaging the Model for TorchServe<a hidden class=anchor aria-hidden=true href=#packaging-the-model-for-torchserve>#</a></h2>
<p>Once the training is complete, we package the model and its dependencies into a .mar file for TorchServe, using the following command:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>python package.py --model_name xlmr_sentiment_traced --version <span class=m>1</span> --handler src/handler.py --requirements dockercontext_torchserve/requirements.txt
</code></pre></div><p>The package script will download the tokenizer and the traced model.</p>
<p>Ones all files are downloaded we will create the .mar file using the following command:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>torch-model-archiver <span class=se>\
</span><span class=se></span>    --force<span class=se>\
</span><span class=se></span>    --model-name sentiment<span class=se>\
</span><span class=se></span>    --version <span class=o>{</span>args.version<span class=o>}</span><span class=se>\
</span><span class=se></span>    --serialized-file <span class=o>{</span>DESTINATION_FOLDER<span class=o>}</span>/traced.pt<span class=se>\
</span><span class=se></span>    --handler <span class=o>{</span>args.handler<span class=o>}</span><span class=se>\
</span><span class=se></span>    --export-path <span class=o>{</span>DESTINATION_FOLDER<span class=o>}</span><span class=se>\
</span><span class=se></span>    --requirements-file <span class=o>{</span>args.requirements<span class=o>}</span><span class=se>\
</span><span class=se></span>    --extra-files <span class=o>{</span>DESTINATION_FOLDER<span class=o>}</span>/special_tokens_map.json,<span class=o>{</span>DESTINATION_FOLDER<span class=o>}</span>/tokenizer_config.json,<span class=o>{</span>DESTINATION_FOLDER<span class=o>}</span>/tokenizer.json
</code></pre></div><p>Here we package the traced model, tokenizer, all dependencies specified in the requirements-file and the handler. The handler is responsible for initialize the model and tokenizer and to setup the function that the API will call when there is a request. The handler implements the functions below:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>class</span> <span class=nc>SentimentHandler</span><span class=p>(</span><span class=n>BaseHandler</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
    <span class=k>def</span> <span class=nf>initialize</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>context</span><span class=p>):</span>
        <span class=c1># Load the model and tokenizer</span>
        <span class=k>pass</span> 
    <span class=k>def</span> <span class=nf>postprocess</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>data</span><span class=p>):</span>
        <span class=c1># Convert the output tensor to a list of predictions</span>
        <span class=k>pass</span>
    <span class=k>def</span> <span class=nf>preprocess</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>requests</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=nb>bytearray</span><span class=p>]]):</span>
        <span class=c1># Process the input text and tokenize it</span>
        <span class=k>pass</span>
    <span class=k>def</span> <span class=nf>inference</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model_input</span><span class=p>):</span>
        <span class=c1># Perform inference using the model and return predictions</span>
        <span class=k>pass</span>
</code></pre></div><h2 id=running-the-pipeline-remotely-with-reacher>Running the Pipeline Remotely with Reacher<a hidden class=anchor aria-hidden=true href=#running-the-pipeline-remotely-with-reacher>#</a></h2>
<p>To run the pipeline on a remote machine (needed for training - if you are working on a laptop), I&rsquo;m using the <a href=https://pypi.org/project/reacher/>Reacher library</a>. First, set up a Reacher instance with your remote configuration:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=kn>from</span> <span class=nn>reacher.reacher</span> <span class=kn>import</span> <span class=n>RemoteClient</span><span class=p>,</span> <span class=n>ReacherDocker</span>
<span class=kn>from</span> <span class=nn>dotenv</span> <span class=kn>import</span> <span class=n>dotenv_values</span>

<span class=n>config</span> <span class=o>=</span> <span class=n>dotenv_values</span><span class=p>()</span>

<span class=n>reacher</span> <span class=o>=</span> <span class=n>ReacherDocker</span><span class=p>(</span>
    <span class=n>build_name</span><span class=o>=</span><span class=s2>&#34;pytorch_base&#34;</span><span class=p>,</span>
    <span class=n>image_name</span><span class=o>=</span><span class=s2>&#34;pytorch_base&#34;</span><span class=p>,</span>
    <span class=n>build_context</span><span class=o>=</span><span class=s2>&#34;dockercontext&#34;</span><span class=p>,</span>
    <span class=n>host</span><span class=o>=</span><span class=n>config</span><span class=p>[</span><span class=s2>&#34;HOST&#34;</span><span class=p>],</span>
    <span class=n>user</span><span class=o>=</span><span class=n>config</span><span class=p>[</span><span class=s2>&#34;USER&#34;</span><span class=p>],</span>
    <span class=n>password</span><span class=o>=</span><span class=n>config</span><span class=p>[</span><span class=s2>&#34;PASSWORD&#34;</span><span class=p>],</span>
    <span class=n>ssh_key_filepath</span><span class=o>=</span><span class=n>config</span><span class=p>[</span><span class=s2>&#34;SSH_KEY_PATH&#34;</span><span class=p>],</span>
<span class=p>)</span>

<span class=n>reacher</span><span class=o>.</span><span class=n>build</span><span class=p>()</span>

<span class=n>reacher</span><span class=o>.</span><span class=n>setup</span><span class=p>(</span>
    <span class=n>ports</span><span class=o>=</span><span class=p>[</span><span class=mi>8888</span><span class=p>,</span> <span class=mi>6666</span><span class=p>],</span>
    <span class=n>envs</span><span class=o>=</span><span class=n>dotenv_values</span><span class=p>(</span><span class=s2>&#34;.env&#34;</span><span class=p>)</span> 
<span class=p>)</span>
</code></pre></div><p>Then, execute the different pipeline steps on the remote machine:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=n>reacher</span><span class=o>.</span><span class=n>execute</span><span class=p>(</span>
    <span class=n>context</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;src&#34;</span><span class=p>],</span>
    <span class=n>command</span><span class=o>=</span><span class=s2>&#34;python &lt;SCRIPT&gt;.py &lt;ARGS&gt;... &#34;</span><span class=p>,</span>
    <span class=n>named_session</span><span class=o>=</span><span class=s2>&#34;&#34;</span><span class=p>,</span>
<span class=p>)</span>
</code></pre></div><p>Here you will use the same commands as running it locally, just specify the file that are needed to run the scripts and those will get uploaded to the remote before the commands is executed.</p>
<p>Ones all steps are done, fetch the .mar file containing the trained model:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=n>reacher</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;artifacts/sentiment.mar&#34;</span><span class=p>,</span> <span class=s2>&#34;artifacts&#34;</span><span class=p>)</span>
</code></pre></div><h1 id=deployment-on-azure-kubernetes-cluster-with-torchserve>Deployment on Azure Kubernetes Cluster with TorchServe<a hidden class=anchor aria-hidden=true href=#deployment-on-azure-kubernetes-cluster-with-torchserve>#</a></h1>
<h2 id=setting-up-the-kubernetes-cluster-on-azure>Setting up the Kubernetes cluster on Azure<a hidden class=anchor aria-hidden=true href=#setting-up-the-kubernetes-cluster-on-azure>#</a></h2>
<p>To set up the Kubernetes cluster on Azure, export environment variables from the .env file:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=nb>export</span> <span class=k>$(</span>cat .env <span class=p>|</span> xargs<span class=k>)</span>
</code></pre></div><p>Create the cluster:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>az aks create --resource-group <span class=si>${</span><span class=nv>AZURE_RESOURCE_GROUP</span><span class=si>}</span> --name <span class=si>${</span><span class=nv>COMPUTE_CLUSTER_NAME</span><span class=si>}</span> --node-vm-size Standard_DC2s_v2 --node-count <span class=m>1</span> --generate-ssh-keys
</code></pre></div><p>Here you can specify a more performant node-size than Standard_DC2s_v2.</p>
<p>Install the necessary tools, such as kubectl:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>az aks install-cli
</code></pre></div><p>Set up the credentials and push them to ~/.kube/config so that kubectl can be used:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>az aks get-credentials --resource-group <span class=si>${</span><span class=nv>AZURE_RESOURCE_GROUP</span><span class=si>}</span>  --name <span class=si>${</span><span class=nv>COMPUTE_CLUSTER_NAME</span><span class=si>}</span>
</code></pre></div><h3 id=configuring-the-cluster>Configuring the cluster<a hidden class=anchor aria-hidden=true href=#configuring-the-cluster>#</a></h3>
<p>If you are using GPU-powered nodes we need to setup nvidia-device-plugin to allow the nodes to access the GPUs.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>kubectl apply -f k8s/nvidia-device-plugin-ds.yaml
</code></pre></div><h3 id=configuring-storage>Configuring storage<a hidden class=anchor aria-hidden=true href=#configuring-storage>#</a></h3>
<p>Create a storage class for files across the nodes:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>kubectl apply -f k8s/Azure_file_sc.yaml
</code></pre></div><p>Create a PersistentVolume to store mar-files and configs:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>kubectl apply -f k8s/AKS_pv_claim.yaml
</code></pre></div><p>Create a pod with PersistentVolume for copying mar-files and config files:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>kubectl apply -f k8s/model_store_pod.yaml
</code></pre></div><p>Check the running pods:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>kubectl get pods -o wide
</code></pre></div><h2 id=uploading-marconfig-files-to-kubernetes>Uploading mar/config files to Kubernetes<a hidden class=anchor aria-hidden=true href=#uploading-marconfig-files-to-kubernetes>#</a></h2>
<p>Create a folder on the pod with PersistentVolume to upload mar/config files:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>kubectl <span class=nb>exec</span> --tty pod/model-store-pod -- mkdir /mnt/azure/model-store/
</code></pre></div><p>Copy the mar file to the pod with PersistentVolume:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>kubectl cp dockercontext_torchserve/model_store/sentiment.mar model-store-pod:/mnt/azure/model-store/sentiment.mar
</code></pre></div><p>Copy the config files:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>kubectl <span class=nb>exec</span> --tty pod/model-store-pod -- mkdir /mnt/azure/config/
kubectl cp dockercontext_torchserve/config.properties model-store-pod:/mnt/azure/config/config.properties
</code></pre></div><p>Check if all the files have been uploaded:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>kubectl <span class=nb>exec</span> --tty pod/model-store-pod -- find /mnt/azure/
</code></pre></div><h2 id=deployment-for-public-access>Deployment for public access<a hidden class=anchor aria-hidden=true href=#deployment-for-public-access>#</a></h2>
<p>Set up a TorchServe deployment and a service that forwards the inference/management/metric ports to the load balancer for external access:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>kubectl create -f k8s/torchserve_public.yaml
</code></pre></div><p>Get information about the TorchServe service:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>kubectl describe service torchserve
</code></pre></div><p>To get information about all services:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>kubectl get service -A
</code></pre></div><p>Use the external IP along with the correct port to start the sentiment model on the deployment:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>curl -v -X POST <span class=s2>&#34;http://&lt;EXTERNAL_IP&gt;:&lt;EXTERNAL_MGM_PORT&gt;/models?initial_workers=1&amp;batch_size=5&amp;maxWorkers=5&amp;max_batch_delay=1000&amp;synchronous=true&amp;url=sentiment.mar&#34;</span>
</code></pre></div><p>Here, we set up one initial worker and allow TorchServe to scale up to 5, if needed. We allow a maximum batch size of 5, and if requests arrive within 1000ms, TorchServe groups them in batches of the maximum size.</p>
<p>To delete the deployment and service:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>kubectl delete deployment torchserve
kubectl delete service torchserve
</code></pre></div><h2 id=deploying-the-model-for-private-access>Deploying the Model for Private Access<a hidden class=anchor aria-hidden=true href=#deploying-the-model-for-private-access>#</a></h2>
<p>To deploy the model for private access, create a pod with the TorchServe image:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>kubectl create -f k8s/torchserve_private.yaml
</code></pre></div><p>Access the TorchServe pod:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>kubectl <span class=nb>exec</span> --stdin --tty torchserve -- /bin/bash
</code></pre></div><p>Now, you can set up and test TorchServe in a similar way as for the public deployment. Change &lt;EXTERNAL_IP> to 127.0.0.1 and use the ports specified in config.properties.</p>
<h2 id=inference-on-the-model>Inference on the Model<a hidden class=anchor aria-hidden=true href=#inference-on-the-model>#</a></h2>
<p>Finally, you can use your deployed model to perform sentiment analysis on text inputs:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>Copy code
curl -X POST -H <span class=s2>&#34;Content-Type: application/json&#34;</span> -d <span class=s1>&#39;[&#34;The movie was so good&#34;, &#34;The movie was so bad&#34;]&#39;</span> http://&lt;EXTERNAL_IP&gt;:&lt;EXTERNAL_INFERENCE_PORT&gt;/v1/models/sentiment:predict
</code></pre></div><p>The output will look like this:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=p>[</span>
  <span class=mf>0.9991590976715088</span><span class=p>,</span>
  <span class=mf>0.0054536801762878895</span>
<span class=p>]</span>
</code></pre></div><p>The model is confident that the first text has a positive sentiment, while the second text has a negative sentiment.</p>
<h2 id=cleaning-up-resources>Cleaning Up Resources<a hidden class=anchor aria-hidden=true href=#cleaning-up-resources>#</a></h2>
<p>To delete the deployment and service, run the following commands:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>kubectl delete deployment torchserve
kubectl delete service torchserve
</code></pre></div><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1>
<p>In this post, I have walked through the entire process of training a LLM for sentiment analysis, packaging it for TorchServe, and deploying it on an Azure Kubernetes cluster. With this, you can now create, train, and deploy your own models for various use cases.</p>
</div>
<footer class=post-footer>
<ul class=post-tags>
</ul>
<nav class=paginav>
<a class=prev href=https://johannes-skog.github.io/post/llm-chat/>
<span class=title>« Prev</span>
<br>
<span>Training Large (+7b) Language Models on Chat Data: Using DeepSpeed and LORA for Efficient Training</span>
</a>
<a class=next href=https://johannes-skog.github.io/post/reacher/>
<span class=title>Next »</span>
<br>
<span>Reacher - reach out to a remote..</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Training & Deploying LLMs: A Step-by-Step Guide on twitter" href="https://twitter.com/intent/tweet/?text=Training%20%26%20Deploying%20LLMs%3a%20A%20Step-by-Step%20Guide&url=https%3a%2f%2fjohannes-skog.github.io%2fpost%2fsentiment-deployment%2f&hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Training & Deploying LLMs: A Step-by-Step Guide on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fjohannes-skog.github.io%2fpost%2fsentiment-deployment%2f&title=Training%20%26%20Deploying%20LLMs%3a%20A%20Step-by-Step%20Guide&summary=Training%20%26%20Deploying%20LLMs%3a%20A%20Step-by-Step%20Guide&source=https%3a%2f%2fjohannes-skog.github.io%2fpost%2fsentiment-deployment%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Training & Deploying LLMs: A Step-by-Step Guide on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fjohannes-skog.github.io%2fpost%2fsentiment-deployment%2f&title=Training%20%26%20Deploying%20LLMs%3a%20A%20Step-by-Step%20Guide"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Training & Deploying LLMs: A Step-by-Step Guide on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fjohannes-skog.github.io%2fpost%2fsentiment-deployment%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Training & Deploying LLMs: A Step-by-Step Guide on whatsapp" href="https://api.whatsapp.com/send?text=Training%20%26%20Deploying%20LLMs%3a%20A%20Step-by-Step%20Guide%20-%20https%3a%2f%2fjohannes-skog.github.io%2fpost%2fsentiment-deployment%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Training & Deploying LLMs: A Step-by-Step Guide on telegram" href="https://telegram.me/share/url?text=Training%20%26%20Deploying%20LLMs%3a%20A%20Step-by-Step%20Guide&url=https%3a%2f%2fjohannes-skog.github.io%2fpost%2fsentiment-deployment%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2023 <a href=https://johannes-skog.github.io/>Johannes Skog</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerHTML='copy';function d(){a.innerHTML='copied!',setTimeout(()=>{a.innerHTML='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>